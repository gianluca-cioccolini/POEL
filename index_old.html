<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Focusing Robot Open-Ended Reinforcement Learning Through Users&rsquo; Purposes</title>
</head>
<body>
    <h1 style="text-align: center;">Focusing Robot Open-Ended Reinforcement Learning Through Users&rsquo; Purposes</h1>
    <h3 style="text-align: center;"><strong>Emilio Cartoni, Gianluca Cioccolini, Gianluca Baldassarre</strong></h3>
    <p style="text-align: center;">Laboratory of Embodied Natural and Artificial Intelligence (LENAI),<br />Institute of Cognitive Sciences and Technologies (ISTC),<br />National Research Council (CNR),<br />Rome, Italy<br />{emilio.cartoni, gianluca.cioccolini, gianluca.baldassarre}@istc.cnr.it</p>
    <p style="text-align: center;"><strong><span style="color: #800000;">Multi-disciplinary Conference on Reinforcement Learning and Decision Making (RLDM), 2025</span></strong></p>
    <p style="text-align: center;"><a title="arXiv" href="https://arxiv.org/abs/2503.12579" target="_blank">arXiv</a> &nbsp;&nbsp;&nbsp; </p>
    
    <div style="text-align: center;">
        <video width="720" controls style="display: inline-block;">
            <source src="videos/SupplementaryVideo.mp4" type="video/mp4">
        </video>
    </div>
    </blockquote> </blockquote>
    </blockquote>
    <blockquote>
    <p>&nbsp;</p>
    <p style="text-align: center;">&nbsp;</p>
    <p style="text-align: center;">&nbsp;</p>
    <p>&nbsp;</p>
    </blockquote>
    </blockquote>

    <h2 style="text-align: center;">Abstract</h2>
    <blockquote>
    <blockquote>
    <blockquote><blockquote>
    <p style="text-align: justify;">Open-Ended Learning (OEL) autonomous robots can acquire new skills and knowledge through direct interaction with their environment, relying on mechanisms such as intrinsic motivations and self-generated goals to guide learning processes. OEL robots are highly relevant for applications as they can autonomously leverage acquired knowledge to perform tasks beneficial to human users in unstructured environments, addressing challenges unforeseen at design time. However, OEL robots face a significant limitation: their openness may lead them to waste time learning information that is irrelevant to tasks desired by specific users. Here, we propose a solution called &lsquo;Purpose-Directed Open-Ended Learning&rsquo; (POEL), based on the novel concept of &lsquo;purpose&rsquo; introduced in previous work. A purpose specifies what users want the robot to achieve. The key insight of this work is that purpose can focus OEL on learning self-generated classes of tasks that, while unknown during autonomous learning (as typical in OEL), involve objects relevant to the purpose. This concept is operationalised in a novel robot architecture capable of receiving a human purpose through speech-to-text, analysing the scene to identify objects, and using a Large Language Model to reason about which objects are purpose-relevant. These objects are then used to bias OEL exploration towards their spatial proximity and to self-generate rewards that favour interactions with them. The solution is tested in a simulated scenario where a camera-arm-gripper robot interacts freely with purpose-related and distractor objects. For the first time, the results demonstrate the potential advantages of purpose-focused OEL over state-of-the-art OEL methods, enabling robots to handle unstructured environments while steering their learning toward knowledge acquisition relevant to users.</p>
    <h2 style="text-align: center;">Method</h2>
    <p style="text-align: center;">
        <img src="images/architettura.png" alt="Architectures POEL" width="900">
    </p>
    <p>&nbsp;</p>
    <ol>
    <li style="text-align: justify;"><strong>User Input:</strong> the user communicates the Purpose to the robot through speech, and this is converted to text via Speech-to-Text.</li>
    <li style="text-align: justify;"><strong> Visual Interpretation</strong>: the visual module processes the simulated environment to identify objects and their locations.</li>
    <li style="text-align: justify;"><strong> LLM Processing</strong>: the LLM receives the user&rsquo;s Purpose and the visual description of the scenario from the visual module, i.e. the list of the detected objects. The LLM interprets the Purpose and determines which objects are relevant to achieving it, giving as output a subset of the objects detected in the scene. This list of objects biases the robot&rsquo;s interactions to favour these objects, guiding the agent to focus on tasks relevant for the Purpose.</li>
    <li style="text-align: justify;"><strong>Initialization</strong>: based on the LLM&rsquo;s interpretation, the robotic arm&rsquo;s initial position is set near a relevant object. This strategic initialization increases the likelihood of the agent interacting with objects pertinent to the user&rsquo;s Purpose, enhancing learning efficiency and later task performance.</li>
    <li style="text-align: justify;"><strong>Purpose Proximity and Lifting rewards</strong>: using the visual module&rsquo;s ability to determine the position of the target objects, both observations coming from the environment and from imagined rollouts internal to LEXA are analysed by the Proximity and Lifting Purpose models and are assigned rewards. This enhances the interactions with Purpose-related objects and the learning of Purpose-relevant tasks.</li>
    </ol>
    <p style="text-align: justify;">&nbsp;</p>
    <p>&nbsp;</p>

    <h2 style="text-align: center;">Environments</h2>
    
    

    <h3 style="text-align: center;">1. IIWA Environment</h3>
    <p style="text-align: justify;"></p>
    In the first environment, a Kuka LBR iiwa R800 robotic arm is positioned at the center of a table.
    In this setup, the robot is surrounded by three cubes (red, green, and blue) and two containers on the ground where it is mounted.
    The robot performance was evaluated with 24 distinct goals, each defined as an image, grouped as follows:
    6 Posture Goals -- get to specific postures with the arm;
    2 Reaching Goals -- reach the blue cube or the green cube with the effector hand;
    8 Pushing Goals -- push the green or the blue cube to a target position;
    8 Pick-and-Place Goals -- pick and place the green or the blue cube into a container.
    </p>
    <p style="text-align: center;">
        <img src="images/goal_reach_blue.png" alt="IIWA environment" width="400">
        <img src="images/goal_push_green.png" alt="Push green cube goal" width="400">
        <img src="images/goal_pick_and_place_blue_cube.png" alt="Pick and place blue cube goal" width="400">
    </p>

    <h3 style="text-align: center;">2. Franka Environment</h3>
    <p style="text-align: justify;"></p>
    The second environment involves a Franka Emika Panda robot placed in front of a simulated kitchen, where a banana, a broccoli, a snack bar, a bowl, and a kitchen sink are positioned on the kitchen worktop. 
    In this scenario, performance was evaluated on 14 distinct goals:
    6 Posture Goals -- get to specific postures with the arm;
    2 Reaching Goals -- reach the banana or the broccoli;
    2 Pushing Goals -- push the banana or the broccoli to a target position;
    2 Placing-in-Sink Goals -- place the banana or the broccoli in the sink;
    2 Placing-in-Bowl Goals -- place the banana or the broccoli in the bowl.
    </p>
    <p style="text-align: center;">
        <img src="images/goal_reach_banana.png" alt="Franka environment" width="400">
        <img src="images/goal_broccoli_in_the_sink.png" alt="Push green cube goal" width="400">
        <img src="images/goal_broccoli_in_the_bowl.png" alt="Pick and place blue cube goal" width="400">
    </p>
    <p>&nbsp;</p>
    <blockquote>
    <blockquote>
    <blockquote><blockquote>


    <h2 style="text-align: center;">Results</h2>
    <h3 style="text-align: center;">IIWA Environment Results</h3>
    <p style="text-align: justify;"></p>
    In the first environment, the robot was evaluated with two purposes: `learn to manipulate blue objects' and `learn to manipulate green objects'. 
    The results obtained were compared against both LEXA and ALAN* 
    </p>
    <p style="text-align: center;">
        <img src="images/Reach blue object_molt.png" alt="Reach blue object" width="400">
        <img src="images/Push blue object_molt.png" alt="Push blue object" width="400">
        <img src="images/Pick and place blue object_molt.png" alt="Pick and place blue object" width="400">
    </p>
    <p style="text-align: center;">
        <img src="images/Reach_green_object.png" alt="Reach green object" width="400">
        <img src="images/Push green object_molt.png" alt="Push green object" width="400">
        <img src="images/Pick and place green object_molt.png" alt="Pick and place green object" width="400">
    </p>

    <h3 style="text-align: center;">Franka Environment Results</h3>
    <p style="text-align: justify;"></p>
    Similarly, in the second environment, we defined two new purposes: `learn to manipulate fruits' and `learn to manipulate vegetables' and evaluated both, again comparing the results with LEXA and ALAN*.
    </p>
    <p style="text-align: center;">
        <img src="images/Reach banana_molt.png" alt="Reach banana" width="300">
        <img src="images/Push banana_molt.png" alt="Push banana" width="300">
        <img src="images/Banana in the sink_molt.png" alt="Banana in the sink" width="300">
        <img src="images/Banana in the bowl_molt.png" alt="Banana in the bowl" width="300">
    </p>
    <p style="text-align: center;">
        <img src="images/Reach broccoli_molt.png" alt="Reach broccoli" width="300">
        <img src="images/Push Broccoli_molt.png" alt="Push broccoli" width="300">
        <img src="images/Broccoli in the sink_molt.png" alt="Broccoli in the sink" width="300">
        <img src="images/Broccoli in the bowl_molt.png" alt="Broccoli in the bowl" width="300">
    </p>

    <h3 style="text-align: center;">Performance Analysis</h3>
    <p style="text-align: justify;">
        Our algorithm significantly outperforms both LEXA and ALAN* in goals where the object being manipulated is directly correlated with the defined purpose. 
        While the POEL systems lost some performance on the unrelated tasks, as expected, their focus enhances the performance on purpose-related tasks.
        Overall, these results highlight the advantage of incorporating purpose-driven exploration and targeted skill acquisition into OEL processes.
    </p>


    </blockquote>
    </blockquote>

    <p>&nbsp;</p>
    <blockquote>
    <blockquote>
    <blockquote><blockquote>


    <h2 style="text-align: center;">Conclusions</h2>
    
    <p style="text-align: justify;"></p>
    These findings highlight a possible means to focus OEL agents towards a subset of the environment, 
    keeping their open-endedness but focusing their learning towards relevant tasks to avoid dissipating 
    resources on irrelevant tasks.
    </p>
    


    
</body>
</html>
