<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <title>Focusing Robot Open-Ended Reinforcement Learning Through Users’ Purposes</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0 15px;
            max-width: 1000px;
            margin-left: auto;
            margin-right: auto;
        }
        h1, h2, h3 {
            text-align: center;
            margin-bottom: 10px;
        }
        p {
            text-align: justify;
        }
        .center {
            text-align: center;
        }
        .authors, .conference {
            text-align: center;
            font-weight: bold;
        }
        .video-container, .image-container {
            text-align: center;
            margin: 20px 0;
        }
        .image-container img {
            margin: 10px;
        }
        ol {
            padding-left: 20px;
        }
    </style>
</head>
<body>

    <h1>Focusing Robot Open-Ended Reinforcement Learning Through Users’ Purposes</h1>

    <h3 class="authors">Emilio Cartoni, Gianluca Cioccolini, Gianluca Baldassarre</h3>
    <p class="center">
        Laboratory of Embodied Natural and Artificial Intelligence (LENAI),<br>
        Institute of Cognitive Sciences and Technologies (ISTC),<br>
        National Research Council (CNR), Rome, Italy<br>
        {emilio.cartoni, gianluca.cioccolini, gianluca.baldassarre}@istc.cnr.it
    </p>
    <p class="conference" style="color: #800000;">Multi-disciplinary Conference on Reinforcement Learning and Decision Making (RLDM), 2025</p>

    <p class="center"><a href="https://arxiv.org/abs/2503.12579" target="_blank">arXiv</a></p>

    <div class="video-container">
        <video width="720" controls>
            <source src="videos/SupplementaryVideo.mp4" type="video/mp4">
        </video>
    </div>

    <h2>Abstract</h2>
    <p>
        Open-Ended Learning (OEL) autonomous robots can acquire new skills and knowledge through direct interaction with their environment, relying on mechanisms such as intrinsic motivations and self-generated goals to guide learning processes. OEL robots are highly relevant for applications as they can autonomously leverage acquired knowledge to perform tasks beneficial to human users in unstructured environments, addressing challenges unforeseen at design time. However, OEL robots face a significant limitation: their openness may lead them to waste time learning information that is irrelevant to tasks desired by specific users. Here, we propose a solution called &lsquo;Purpose-Directed Open-Ended Learning&rsquo; (POEL), based on the novel concept of &lsquo;purpose&rsquo; introduced in previous work. A purpose specifies what users want the robot to achieve. The key insight of this work is that purpose can focus OEL on learning self-generated classes of tasks that, while unknown during autonomous learning (as typical in OEL), involve objects relevant to the purpose. This concept is operationalised in a novel robot architecture capable of receiving a human purpose through speech-to-text, analysing the scene to identify objects, and using a Large Language Model to reason about which objects are purpose-relevant. These objects are then used to bias OEL exploration towards their spatial proximity and to self-generate rewards that favour interactions with them. The solution is tested in a simulated scenario where a camera-arm-gripper robot interacts freely with purpose-related and distractor objects. For the first time, the results demonstrate the potential advantages of purpose-focused OEL over state-of-the-art OEL methods, enabling robots to handle unstructured environments while steering their learning toward knowledge acquisition relevant to users.
    </p>

    <h2>Method</h2>
    <div class="image-container">
        <img src="images/architettura.png" alt="Architectures POEL" width="900">
    </div>

    <ol>
        <li><strong>User Input:</strong> the user communicates the Purpose to the robot through speech, and this is converted to text via Speech-to-Text.</li>
        <li><strong>Visual Interpretation:</strong> the visual module processes the simulated environment to identify objects and their locations.</li>
        <li><strong>LLM Processing:</strong> the LLM receives the user&rsquo;s Purpose and the visual description of the scenario from the visual module, i.e. the list of the detected objects. The LLM interprets the Purpose and determines which objects are relevant to achieving it, giving as output a subset of the objects detected in the scene. This list of objects biases the robot&rsquo;s interactions to favour these objects, guiding the agent to focus on tasks relevant for the Purpose.</li>
        <li><strong>Initialization:</strong> based on the LLM&rsquo;s interpretation, the robotic arm&rsquo;s initial position is set near a relevant object. This strategic initialization increases the likelihood of the agent interacting with objects pertinent to the user&rsquo;s Purpose, enhancing learning efficiency and later task performance.</li>
        <li><strong>Purpose Rewards:</strong> using the visual module&rsquo;s ability to determine the position of the target objects, both observations coming from the environment and from imagined rollouts internal to LEXA are analysed by the Proximity and Lifting Purpose models and are assigned rewards. This enhances the interactions with Purpose-related objects and the learning of Purpose-relevant tasks.</li>
    </ol>

    <h2>Environments</h2>

    <h3>1. IIWA Environment</h3>
    <p>
        A Kuka LBR iiwa R800 arm on a table with 3 colored cubes and 2 containers. Evaluated on 24 goals including postures, reaching, pushing, and pick-and-place tasks.
    </p>
    <div class="image-container">
        <img src="images/goal_reach_blue.png" width="300">
        <img src="images/goal_push_green.png" width="300">
        <img src="images/goal_pick_and_place_blue_cube.png" width="300">
    </div>

    <h3>2. Franka Environment</h3>
    <p>
        A Franka Emika Panda robot placed in front of a simulated kitchen, where a banana, a broccoli, a snack bar, a bowl, and a kitchen sink are positioned on the kitchen worktop. Evaluated on 14 goals including posture, reaching, pushing, and placing objects in sink/bowl.
    </p>
    <div class="image-container">
        <img src="images/goal_reach_banana.png" width="300">
        <img src="images/goal_broccoli_in_the_sink.png" width="300">
        <img src="images/goal_broccoli_in_the_bowl.png" width="300">
    </div>

    <h2>Results</h2>
    <h3>IIWA Environment</h3>
    <p>Tested with purposes: "learn to manipulate blue objects" and "learn to manipulate green objects". Compared against LEXA and ALAN*.</p>
    <div class="image-container">
        <img src="images/Reach blue object_molt.png" width="300">
        <img src="images/Push blue object_molt.png" width="300">
        <img src="images/Pick and place blue object_molt.png" width="300">
        <img src="images/Reach_green_object.png" width="300">
        <img src="images/Push green object_molt.png" width="300">
        <img src="images/Pick and place green object_molt.png" width="300">
    </div>

    <h3>Franka Environment</h3>
    <p>Tested with purposes: "learn to manipulate fruits" and "learn to manipulate vegetables". Compared against LEXA and ALAN*.</p>
    <div class="image-container">
        <img src="images/Reach banana_molt.png" width="220">
        <img src="images/Push banana_molt.png" width="220">
        <img src="images/Banana in the sink_molt.png" width="220">
        <img src="images/Banana in the bowl_molt.png" width="220">
        <img src="images/Reach broccoli_molt.png" width="220">
        <img src="images/Push Broccoli_molt.png" width="220">
        <img src="images/Broccoli in the sink_molt.png" width="220">
        <img src="images/Broccoli in the bowl_molt.png" width="220">
    </div>

    <h3>Performance Analysis</h3>
    <p>
        The POEL approach outperformed LEXA and ALAN* on purpose-related tasks, highlighting its efficiency in focused learning while maintaining general exploration capabilities.
    </p>

    <h2>Conclusions</h2>
    <p>
        These findings highlight a possible means to focus OEL agents towards a subset of the environment, 
    keeping their open-endedness but focusing their learning towards relevant tasks to avoid dissipating 
    resources on irrelevant tasks.
    </p>

</body>
</html>
